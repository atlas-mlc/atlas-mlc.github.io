<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Mixture of Experts (MoE) | </title>
<meta name="keywords" content="first" />
<meta name="description" content="MoE are rumored to be a critical components in scaling up to a trillion parameter model. By routing tokens to specialized modular functions, it enables models to have representational power of a much larger model than what is used for prediction. We will be discussing how MoE works and its recent advances in this literature">
<meta name="author" content="
Author: &nbspJyo Pari
&nbsp | &nbsp Editor: &nbspN/A">
<link rel="canonical" href="https://canonical.url/to/page" />
<link crossorigin="anonymous" href="https://minyoungg.github.io/mlscale/assets/css/stylesheet.min.012b9a75b78c05a5b3111bea2ffe80bb9c7a658de709126230287fb6db7016a7.css" integrity="" rel="preload stylesheet" as="style">
<link rel="icon" href="https://minyoungg.github.io/mlscale/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://minyoungg.github.io/mlscale/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://minyoungg.github.io/mlscale/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://minyoungg.github.io/mlscale/apple-touch-icon.png">
<link rel="mask-icon" href="https://minyoungg.github.io/mlscale/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.119.0">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Mixture of Experts (MoE)" />
<meta property="og:description" content="MoE are rumored to be a critical components in scaling up to a trillion parameter model. By routing tokens to specialized modular functions, it enables models to have representational power of a much larger model than what is used for prediction. We will be discussing how MoE works and its recent advances in this literature" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://minyoungg.github.io/mlscale/posts/moe.html" /><meta property="og:image" content="https://minyoungg.github.io/mlscale/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-04T11:30:03&#43;00:00" />
<meta property="article:modified_time" content="2023-10-04T11:30:03&#43;00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://minyoungg.github.io/mlscale/papermod-cover.png"/>

<meta name="twitter:title" content="Mixture of Experts (MoE)"/>
<meta name="twitter:description" content="MoE are rumored to be a critical components in scaling up to a trillion parameter model. By routing tokens to specialized modular functions, it enables models to have representational power of a much larger model than what is used for prediction. We will be discussing how MoE works and its recent advances in this literature"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Mixture of Experts (MoE)",
      "item": "https://minyoungg.github.io/mlscale/posts/moe.html"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Mixture of Experts (MoE)",
  "name": "Mixture of Experts (MoE)",
  "description": "MoE are rumored to be a critical components in scaling up to a trillion parameter model. By routing tokens to specialized modular functions, it enables models to have representational power of a much larger model than what is used for prediction. We will be discussing how MoE works and its recent advances in this literature",
  "keywords": [
    "first"
  ],
  "articleBody": " MoE Introduction Mixture of Experts (MoE) have gotten popular recently with the rise of large language models and multi modal reasoning. They are not a new idea, and have existed for a while in the form of Ensemble Methods. For example, you might have heard of Bagging and Boosting. Bagging refers to training different models on different random partitions of data and then aggregate their results to produce a more robust model. Boosting involves training models sequentially, and each consecutive model is trained on a reweighed data depending on the previous models performance. In addition, you probably have seen models like Gaussian Mixture Models. While they are simple, they capture the essence of the motivation for a mixture model, model a more complex distribution through explicit use of simple distributions / functions.\nhttps://www.youtube.com/watch?v=U8J32Z3qV8s Key Papers To be transparent, most of the papers I choose are the ones that Finbarr Timbers used it his awesome blogs on MoE, make sure to check his page out! He seemed to already capture the key ideas, but hopefully I added some extra insights.\nOUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER They present a model in the form\n$$ y = \\sum_{i=1}^nG(x)_iE_i(x) \\\\ \\text{where}, G(x) = \\text{Softmax}(\\text{KeepTopK}(H(x), k)) \\\\ H(x)_i = (xWg)_i+N(0,1)*\\text{Softplus}((xW_{noise})_i) \\\\ \\text{KeepTopK}(v,k)_i = v_i\\,\\, \\text{if}\\,\\,v_i\\in\\{\\text{top k}\\}\\,\\, \\text{else}\\,\\, -\\infty $$ Both \\( W_g, W_{\\text{noise}} \\) are learned through normal back propogation. I think a important takeaway is the \\( W_{\\text{noise}} \\) parameters, because it allows for exploration, but under expectation, once \\(E_i(x)\\) converge to their optimal functions, then \\(W_\\text{noise}\\) should converge to zero as \\(Wg_i\\) converges to the optimal value. In addition \\(k\\) should be larger than one, because the SwitchFormer authors write that Shazeer et al. (2017) conjectured that routing to k \u003e 1 experts was necessary in order to have non-trivial gradients to the routing functions. The authors intuited that learning to route would not work without the ability to compare at least two experts.\nThe authors convey that problem that often arises in these setups is\nWe have observed that the gating network tends to converge to a state where it always produces large weights for the same few experts. This imbalance is self-reinforcing, as the favored experts are trained more rapidly and thus are selected even more by the gating network. Eigen et al. (2013) describe the same phenomenon, and use a hard constraint at the beginning of training to avoid this local minimum. Bengio et al. (2015) include a soft constraint on the batch-wise average of each gate.\nTo mitigate this issue, they introduce a Importance loss term that tries to enforce a higher variation of the gating value **over a batch** \\( X \\) .\n$$ \\text{Importance}(X) = \\sum_{x\\in X}G(x) \\\\ L_\\text{importance}(X) \\propto CV(\\text{Importance}(X))^2 $$ Where CV is the coefficient of variation \\(\\sigma/\\mu\\). This encourages the model to have uniform gating across a batch. However, is still not computationally ideal because of the following reason. The authors write that We want to define an additional loss function to encourage experts to receive roughly equal numbers of training examples. Unfortunately, the number of examples received by an expert is a discrete quantity, so it can not be used in backpropagation.\nThis also helps in a distributed setup, where computationally is more evenly spread across.\nThe problem with the \\( \\text{Importance} \\) loss term is that, as you sum across the batch you loose information of the gate values for individual data points in the batch, this information loss is why the aforementioned problem arises. So the authors have a new metric Let \\(P(x,i)\\) denote the probability that *“probability that \\(G(x)_i\\) is nonzero, given a new random choice of noise on element \\(i\\), but keeping the already-sampled choices of noise on the other elements”.* And they create an additional loss term which will spread apart values per each column which will prevent the degenerate case that the Importance term can suffer from. $$ \\text{Load}(X)_i = \\sum_{x\\in X}P(x,i) \\\\ L_{\\text{load}}(X) \\propto CV(\\text{Load}(X))^2 $$ Something I am not sure about is why we can’t just use the Load loss term and drop the Importance term. One final detail is they set \\(W_{\\text{noise}}, W_g\\) to all zeros because that will have a uniform weighting over the experts initially, which helps with allowing them to specialize. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity Fig 2 from the paper, note the routing function only looks at its current token and its independent of previous tokens Each token is routed to one expert. So this different than previous works that show that tokens should be routed for multiple experts. The authors show that this is no longer the case and this also improves computational efficiency.\nFigure 3 from paper In this figure we see that for a batch of tokens, each expert has a budget of how many tokens they can process in total. This is denoted by the Expert Capacity. If its too small, then some tokens won’t be processed (the blue one in the left picture), however, too large of a capacity is also inefficient.\nFor their load balancing loss, let \\(N\\) be the number of experts, and $B$ be the batch that has \\(T\\) tokens. The loss is: $$ loss = \\alpha N \\sum_{i=1}^Nf_iP_i, \\\\ f_i = \\frac{1}{T}\\sum_{x\\in B}\\mathbf{1}\\{\\text{argmax}\\, p(x) = i\\}, \\mathbf{1} \\text{ is the indicator function}\\\\ P_i = \\frac{1}{T}\\sum_{x\\in B}p_i(x) $$ One neat thing is with this loss, you don’t need to have both a load balancing and importance loss as the previous paper had. Lets unpack what this loss is doing. Since \\(f_i\\) will roughly be aligned with \\(P_i\\) then we can say in a handy wavy way that the loss can be minimized when both are uniform. This also prevents cases where \\(p_i(x)\\) is very unimodal because for the same vector \\(F = \\{f\\}_i^N\\), there can be mutiple \\(p_{1-n}(x)\\), so the uniform \\(p\\) would minmize the loss the most. Importantly, the authors show that there is consistently an improvement when adding more experts and this is done with the same computational budget, see figure to the right. And they also show the scaling is better than the traditional dense scaling. See figure below.\nPart of figure 4 from the paper Figure 6 from paper Hash Layers For Large Sparse Models The MoE layers are implemented to replace the feed forward networks in original transformer, SwitchFormer style. Most papers replace the FFN with the MoE layers because FFNs are ‘’the most computationally expensive part in a Transformer-based network” - (Zhou et al. 2022).\nI found this paper pretty surprising because you can get good performance with a random mapping between the token and which FFN it gets routed to. This seems counter intuitive because one would expect that a dynamic routing model that is able to decide which expert to send the token to depending on the token’s embedding would provide for more flexibility. The authors write:\nWe are free to choose from various possible hash functions, which we will consider below. However, for training purposes, the hash function is fixed in advance, and in this way, our routing mechanism requires no training and has no adjustable parameters …\nSo one problem with this is, because of the Zipfain distribution, which well models the distributions of word frequencies, the distribution of experts being used will also be skewed. So they came up with a Balanced Hash, which uses the distribution of the training data and tries to rehash to obtain a less skewed distribution over the hash buckets. Another version is the Clustered Hash, which performs k-means on the token embeddings, this will hash similar tokens to the same function. Interestingly they also try the opposite of this where within a cluster from k-means, they will spread out the tokens within that cluster over the buckets. The authors motivation for this is:\nvery similar tokens need fine distinctions which requires more model capacity (hence assigning to different experts)\nOne final version they try is to hash part of the weight matrix for the feed forward network: \\(B(\\text{relu}(A(h)))\\). $$ v = \\text{relu}([A_{k_1}(h), ..., A_{k_N}(h)]), FFN(h) = [B_{k_1}(v), ..., B_{k_n}(v)] $$ Where, \\(k_i\\) is determined by the hash: \\(k_i = \\text{hash}_i(x)\\) DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning So in most of the other works in this page we often use a top k select over the gating values. The authors of this paper propose that that could lead to instabilities during training, because the loss landscape is no longer smooth. So to reiterate, the prior MoE models often are equivalent to solving:\n$$ \\underset{f_1, ..., f_n, w}{\\min}\\frac{1}{N}\\sum_{(x,y)\\in D}\\ell(y, \\sum_{i=1}^{n}f_i(x)w_i)\\\\ \\text{s.t.}\\,\\,\\, ||w||_0\\leq k \\\\ \\sum_{i=1}^n w_i = 1, w \\geq 0 $$ So here the \\(L_0\\) norm constraint is what makes it difficult for our usual gradient based optimizers. Consequently, the contribution of this work is to convert this into a unconstrained optimization problem. Pretty cool! $$ r(z)_i = \\prod_{j\\in B(i-1)}(z_j)\\prod_{j\\in [m] \\backslash B(i-1)}(1-z_j) $$ This formula can map binary numbers to one hot vectors. For example, if \\(z = [1,0]\\) then \\(r(z)_2 = 1\\), since I am using 0 indexing. Thus we can use this to obtain a mixture over k experts with a stack of k binary numbers which is \\(Z\\). $$ q(\\alpha, Z) = \\sum_{i=1}^k\\sigma(\\alpha)_ir(z^{(i)}) $$ So our new optimization problem becomes:\n$$ \\underset{f_1, ..., f_n, \\alpha, Z}{min} \\frac{1}{N}\\sum_{(x,y)\\in D} \\ell(y, \\sum_{i]1}^nf_i(x)q(\\alpha, Z)_i)\\\\ z^{(i)}\\in \\{0,1\\}^m, i\\in[k] $$ But this is still not that useful because \\(z^{(i)}\\) is still a binary vector which becomes a combinatorial optimization problem which is not what we want. So instead lets relax \\(z^{(i)}\\) to be continuous and we can do that with the following. \\(S(t)\\) smooth function that can exactly equal 0, 1. $$ \\tilde{q}(\\alpha, Z) \\coloneqq q(\\alpha, S(Z)) = \\sum_{i=1}^k\\sigma(\\alpha)_ir(S(z^{(i)})) $$ $$ \\underset{f_1, ..., f_n, \\alpha, Z}{min} \\frac{1}{N}\\sum_{(x,y)\\in D} \\ell(y, \\sum_{i=1}^nf_i(x)\\tilde{q}(\\alpha, Z)_i) + \\lambda \\Omega(Z)\\\\ $$ The entropy isn’t directly calculated on \\(Z\\) but \\(\\Omega(Z)\\coloneqq \\sum_{i=1}^kh(r(S(z^{(i)})))\\), where $h$ is an entropy function. The authors state that the entropy regularization isn’t needed because empirically the \\(z^{(i)}\\) will become a binary vector, but for faster convergence the entropy term helps. So \\(\\alpha, Z\\) does not depend on \\(x\\) , but you can easily do that as well via a linear transformation. BASE Layers: Simplifying Training of Large, Sparse Models This paper has a similar setup to the previous paper with some key differences. So to go over their notation, they have \\(E\\) experts, and each one is denoted by \\(f_e\\) and its learnable representation \\(w_e\\in \\mathbb{R}^D\\) to allow us to to routing. \\(h_t\\) is the token embedding and \\(a_t \\in \\{0,..., E\\}\\) is the assignment of the token to expert. So the overall model takes the following form: $$ \\sigma(h_t\\cdot w_{a_t})f_{a_t}(h_t) +h_t $$ The assignment during training and testing is different: the authors write that\nDuring training, we maximize model throughput by assigning an equal number of tokens to each expert. At test time, we simply assign each token to its highest scoring expert.\nSo during training they solve the well studied assignment problem\n$$ \\text{maximize}\\sum_th_t\\cdot w_{a_t}\\\\ s.t. \\forall e \\sum_{t=1}^T \\mathbb{1}_{a_t=e} = \\frac{T}{E} $$ Here \\(T\\) is the number of tokens. One potential algorithm to solve this is the famous Hungarian matching one, but that is \\(O(n^3)\\) and not parallelizable. Instead the authors use a different Auction Algorithm (Bertsekas et al. 1922) which is “which is more easily parallelizable on GPUs than the Hungarian Algorithm”. One important point is that since the partition of the dataset over workers would not be IID, they randomly shuffle the tokens across workers before calculating the assignments.\nMixture-of-Experts with Expert Choice Routing The authors start off by highlighting some of the previous problems with MoE models where the routing function decides which for each token, which expert to route it to. These problems revolve around load imbalance, and as we have seen so far, there are multiple different heuristics / regularizations to encourage more uniform load over the experts. So to be concise, here is how they implement their method:\n$$ S = \\text{softmax}(XW_g), S\\in\\mathbb{R}^{n\\times e} \\\\ G,I = \\text{TopK}(S^T, k), P = \\text{Onehot(I)} $$ \\(X \\in \\mathbb{R}^{n \\times d}\\), and \\(I\\) is an index matrix, and G is the weights of the selection. So then a permutation matrix \\(P \\in \\mathbb{R}^{e \\times k \\times n}\\) is calculated based on \\(I\\) to reshuffle the indicies such that \\(X_{in} \\in \\mathbb{R}^{e \\times k \\times d}= PX\\) allows you to index the tokens per expert. Then the FFN and reverse shuffle/weighting is defined as: $$ \\forall i, X_e[i] = \\text{GeLU}(X_{in}[i]W_1[i])W_2[i]^T\\\\ X_{out}[l,d] = \\sum_{i,j}P[i,j,l]G[i,j]X_e[i,j,d] $$ Notice here that there is nothing to prevent experts taking in the same tokens. While it seems that a more even spread of tokens better utilizes the model’s capacity, they introduce a entropy regularization in the form of: $$ \\max_{A}{S^T \\cdot A + \\lambda H(A)}\\\\ \\text{s.t}\\,\\,\\, \\forall i: \\sum_{j'}A[i,j']=k; \\,\\,\\, \\forall j: \\sum_{i'}A[i',j]\\leq b; \\forall i,j: \\,\\, 0\\leq A[i,j]\\leq 1 $$ Where \\(H(A)\\) calculates the entropy. So this is a entropy regularized linear program, and after obtaining \\(A\\) , instead of \\(S^T\\) the top k is calculated using \\(A\\). So what this allows is, if \\(S^T’\\)s top k are rather skewed towards a small set of tokens, depending on how much larger the top k values are compared to the rest, and \\(lambda\\), A can reselect a more uniform set of tokens. ",
  "wordCount" : "2228",
  "inLanguage": "en",
  "datePublished": "2023-10-04T11:30:03Z",
  "dateModified": "2023-10-04T11:30:03Z",
  "author":{
    "@type": "Person",
    "name": "Jyo Pari"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://minyoungg.github.io/mlscale/posts/moe.html"
  },
  "publisher": {
    "@type": "Organization",
    "name": "",
    "logo": {
      "@type": "ImageObject",
      "url": "https://minyoungg.github.io/mlscale/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://minyoungg.github.io/mlscale/">Home</a></div>
    <h1 class="post-title">
      Mixture of Experts (MoE)
    </h1>
    <div class="post-description">
      MoE are rumored to be a critical components in scaling up to a trillion parameter model. By routing tokens to specialized modular functions, it enables models to have representational power of a much larger model than what is used for prediction. We will be discussing how MoE works and its recent advances in this literature
    </div>
    <div class="post-meta"><span title='2023-10-04 11:30:03 +0000 +0000'>October 4, 2023</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;
Author: &nbspJyo Pari
&nbsp | &nbsp Editor: &nbspN/A

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#moe" aria-label="MoE">MoE</a><ul>
                        
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#key-papers" aria-label="Key Papers">Key Papers</a><ul>
                        
                <li>
                    <a href="#outrageously-large-neural-networks-the-sparsely-gated-mixture-of-experts-layerhttpsarxivorgpdf170106538pdf" aria-label="OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER"><a href="https://arxiv.org/pdf/1701.06538.pdf">OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER</a></a></li>
                <li>
                    <a href="#switch-transformers-scaling-to-trillion-parameter-models-with-simple-and-efficient-sparsityhttpsarxivorgpdf210103961pdf" aria-label="Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"><a href="https://arxiv.org/pdf/2101.03961.pdf">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a></a></li>
                <li>
                    <a href="#hash-layers-for-large-sparse-modelshttpsarxivorgpdf210604426pdf" aria-label="Hash Layers For Large Sparse Models"><a href="https://arxiv.org/pdf/2106.04426.pdf">Hash Layers For Large Sparse Models</a></a></li>
                <li>
                    <a href="#dselect-k-differentiable-selection-in-the-mixture-of-experts-with-applications-to-multi-task-learninghttpsarxivorgabs210603760" aria-label="DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning"><strong><a href="https://arxiv.org/abs/2106.03760">DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning</a></strong></a></li>
                <li>
                    <a href="#base-layers-simplifying-training-of-large-sparse-modelshttpsproceedingsmlrpressv139lewis21ahtml" aria-label="BASE Layers: Simplifying Training of Large, Sparse Models"><strong><a href="https://proceedings.mlr.press/v139/lewis21a.html">BASE Layers: Simplifying Training of Large, Sparse Models</a></strong></a></li>
                <li>
                    <a href="#mixture-of-experts-with-expert-choice-routinghttpsarxivorgpdf220209368pdf" aria-label="Mixture-of-Experts with Expert Choice Routing"><a href="https://arxiv.org/pdf/2202.09368.pdf">Mixture-of-Experts with Expert Choice Routing</a></a>
                </li>
            </ul>
            </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content">


<!-- KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>


<h1 id="moe">MoE<a hidden class="anchor" aria-hidden="true" href="#moe">#</a></h1>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p><em>Mixture of Experts</em> (MoE) have gotten popular recently with the rise of large language models and multi modal reasoning. They are not a new idea, and have  existed for a while in the form of Ensemble Methods. For example, you might have heard of <strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>Bagging</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong> and <strong><strong><strong><strong><strong><strong><strong><strong><strong>Boosting.</strong></strong></strong></strong></strong></strong></strong></strong></strong> Bagging refers to training different models on different random partitions of data and then aggregate their results to produce a more robust model. Boosting involves training models sequentially, and each consecutive model is trained on a reweighed data depending on the previous models performance. In addition, you probably have seen models like Gaussian Mixture Models. While they are simple, they capture the essence of the motivation for a mixture model, model a more complex distribution through explicit use of simple distributions / functions.</p>
<p class="cssclass1 cssclass2">
  <img src="./images/MoE/Screenshot_2023-10-02_at_8.56.13_AM.png" alt="Logo" title="Logo title">
  <p style="color:grey;font-size: 1vw;">https://www.youtube.com/watch?v=U8J32Z3qV8s</em>
</p>
<h2 id="key-papers">Key Papers<a hidden class="anchor" aria-hidden="true" href="#key-papers">#</a></h2>
<p>To be transparent, most of the papers I choose are the ones that Finbarr Timbers used it his awesome <a href="https://substack.com/profile/1592028-finbarr-timbers">blogs</a> on MoE, make sure to check his page out! He seemed to already capture the key ideas, but hopefully I added some extra insights.</p>
<h3 id="outrageously-large-neural-networks-the-sparsely-gated-mixture-of-experts-layerhttpsarxivorgpdf170106538pdf"><a href="https://arxiv.org/pdf/1701.06538.pdf">OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER</a><a hidden class="anchor" aria-hidden="true" href="#outrageously-large-neural-networks-the-sparsely-gated-mixture-of-experts-layerhttpsarxivorgpdf170106538pdf">#</a></h3>
<p>They present a model in the form</p>
<div>
$$
y = \sum_{i=1}^nG(x)_iE_i(x) \\
\text{where}, G(x) = \text{Softmax}(\text{KeepTopK}(H(x), k)) \\
H(x)_i = (xWg)_i+N(0,1)*\text{Softplus}((xW_{noise})_i) \\
\text{KeepTopK}(v,k)_i = v_i\,\, \text{if}\,\,v_i\in\{\text{top k}\}\,\, \text{else}\,\, -\infty 
$$
</div>
<p>
Both \( W_g, W_{\text{noise}} \) are learned through normal back propogation. I think a important takeaway is the  \( W_{\text{noise}} \) parameters, because it allows for exploration, but under expectation, once \(E_i(x)\)  converge to their optimal functions, then \(W_\text{noise}\)  should converge to zero as \(Wg_i\) converges to the optimal value. In addition \(k\) should be larger than one, because the SwitchFormer authors write that
</p>
<blockquote>
<p>Shazeer et al. (2017) conjectured that routing to k &gt; 1 experts was necessary in order to have non-trivial gradients to the routing functions. The authors intuited that learning to route would not work without the ability to compare at least two experts.</p>
</blockquote>
<p>The authors convey that problem that often arises in these setups is</p>
<blockquote>
<p>We have observed that the gating network tends to converge to a state where it always produces large weights for the same few experts. This imbalance is self-reinforcing, as the favored experts are trained more rapidly and thus are selected even more by the gating network. Eigen et al. (2013) describe the same phenomenon, and use a hard constraint at the beginning of training to avoid this local minimum. Bengio et al. (2015) include a soft constraint on the batch-wise average of each gate.</p>
</blockquote>
<p> To mitigate this issue, they introduce a Importance loss term that tries to enforce a higher variation of the gating value **over a batch** \( X \) .</p> 
<div>
$$
\text{Importance}(X) = \sum_{x\in X}G(x) \\ 
L_\text{importance}(X) \propto CV(\text{Importance}(X))^2
$$
</div>
<p>
Where CV is the coefficient of variation \(\sigma/\mu\). This encourages the model to have uniform gating across a batch. However, is still not computationally ideal because of the following reason. 
</p>
The authors write that
<blockquote>
<p>We want to define an additional loss function to encourage experts to receive roughly equal numbers of training examples. Unfortunately, the number of examples received by an expert is a discrete quantity, so it can not be used in backpropagation.</p>
</blockquote>
<p>This also helps in a distributed setup, where computationally is more evenly spread across.</p>
<p>
The problem with the \( \text{Importance} \) loss term is that, as you sum across the batch you loose information of the gate values for individual data points in the batch, this information loss is why the aforementioned problem arises. So the authors have a new metric 
</P>
<p>
Let \(P(x,i)\) denote the probability that *“probability that \(G(x)_i\) is nonzero, given a new random choice of noise on element \(i\), but keeping the already-sampled choices of noise on the other elements”.* And they create an additional loss term which will spread apart values per each column which will prevent the degenerate case that the Importance term can suffer from. 
</p>
<div>
$$
\text{Load}(X)_i = \sum_{x\in X}P(x,i) \\
L_{\text{load}}(X) \propto CV(\text{Load}(X))^2
$$
</div>
<p>
Something I am not sure about is why we can’t just use the Load loss term and drop the Importance term. One final detail is they set \(W_{\text{noise}}, W_g\) to all zeros because that will have a uniform weighting over the experts initially, which helps with allowing them to specialize.  
</p>
<h3 id="switch-transformers-scaling-to-trillion-parameter-models-with-simple-and-efficient-sparsityhttpsarxivorgpdf210103961pdf"><a href="https://arxiv.org/pdf/2101.03961.pdf">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a><a hidden class="anchor" aria-hidden="true" href="#switch-transformers-scaling-to-trillion-parameter-models-with-simple-and-efficient-sparsityhttpsarxivorgpdf210103961pdf">#</a></h3>
<p class="cssclass1 cssclass2">
  <img src="./images/MoE/Screenshot_2023-09-25_at_12.53.20_PM.png" alt="Logo" title="Logo title">
  <p style="color:grey;font-size: 1vw;">Fig 2 from the paper, note the routing function only looks at its current token and its independent of previous tokens </em>
</p>
<p>Each token is routed to one expert. So this different than previous works that show that tokens should be routed for multiple experts. The authors show that this is no longer the case and this also improves computational efficiency.</p>
<p class="cssclass1 cssclass2">
  <img src="./images/MoE/Screenshot_2023-09-26_at_10.17.55_AM.png" alt="Logo" title="Logo title">
  <p style="color:grey;font-size: 1vw;">Figure 3 from paper</em>
</p>
<p>In this figure we see that for a batch of tokens, each expert has a budget of how many tokens they can process in total. This is denoted by the <strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>Expert Capacity.</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong> If its too small, then some tokens won’t be processed (the blue one in the left picture), however, too large of a capacity is also inefficient.</p>
<p>
For their load balancing loss, let \(N\) be the number of experts, and $B$ be the batch that has \(T\)  tokens. The loss is: 
</p>
<div>
$$
loss = \alpha N \sum_{i=1}^Nf_iP_i, \\
f_i = \frac{1}{T}\sum_{x\in B}\mathbf{1}\{\text{argmax}\, p(x) = i\}, \mathbf{1} \text{   is the indicator function}\\
P_i = \frac{1}{T}\sum_{x\in B}p_i(x)
$$
</div>
<p>
One neat thing is with this loss, you don’t need to have both a load balancing and importance loss as the previous paper had. Lets unpack what this loss is doing. Since \(f_i\) will roughly be aligned with \(P_i\) then we can say in a handy wavy way that the loss can be minimized when both are uniform. This also prevents cases where \(p_i(x)\)  is very unimodal because for the same vector \(F = \{f\}_i^N\), there can be mutiple \(p_{1-n}(x)\), so the uniform \(p\)  would minmize the loss the most. 
</p>
<p>Importantly, the authors show that there is consistently an improvement when adding more experts and this is done with the same computational budget, see figure to the right.  And they also show the scaling is better than the traditional dense scaling. See figure below.</p>
<p class="cssclass1 cssclass2">
  <img src="./images/MoE/Screenshot_2023-09-26_at_11.47.38_AM.png" alt="Logo" title="Logo title">
  <p style="color:grey;font-size: 1vw;">Part of figure 4 from the paper</em>
</p>
<p class="cssclass1 cssclass2">
  <img src="./images/MoE/Screenshot_2023-09-26_at_11.50.38_AM.png" alt="Logo" title="Logo title">
  <p style="color:grey;font-size: 1vw;">Figure 6 from paper</em>
</p>
<h3 id="hash-layers-for-large-sparse-modelshttpsarxivorgpdf210604426pdf"><a href="https://arxiv.org/pdf/2106.04426.pdf">Hash Layers For Large Sparse Models</a><a hidden class="anchor" aria-hidden="true" href="#hash-layers-for-large-sparse-modelshttpsarxivorgpdf210604426pdf">#</a></h3>
<p>The MoE layers are implemented to replace the feed forward networks in original transformer, SwitchFormer style. Most papers replace the FFN with the MoE layers because FFNs are ‘’the most computationally expensive part in a Transformer-based network” - (Zhou et al. 2022).</p>
<p class="cssclass1 cssclass2">
  <img src="./images/MoE/Screenshot_2023-09-30_at_12.04.45_PM.png" alt="Logo" title="Logo title">
  <p style="color:grey;font-size: 1vw;"></em>
</p>
<p>I found this paper pretty surprising because you can get good performance with a random mapping between the token and which FFN it gets routed to. This seems counter intuitive because one would expect that a dynamic routing model that is able to decide which expert to send the token to depending on the token’s embedding would provide for more flexibility. The authors write:</p>
<blockquote>
<p>We are free to choose from various possible hash functions, which we will consider below. However, for training purposes, the hash function is fixed in advance, and in this way, our routing mechanism requires no training and has no adjustable parameters …</p>
</blockquote>
<p>So one problem with this is, because of the Zipfain distribution, which well models the distributions of word frequencies, the distribution of experts being used will also be skewed. So they came up with a <strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>Balanced Hash</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong>, which uses the distribution of the training data and tries to rehash to obtain a less skewed distribution over the hash buckets. Another version is the <strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>Clustered Hash,</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong> which performs k-means on the token embeddings, this will hash similar tokens to the same function. Interestingly they also try the opposite of this where within a cluster from k-means, they will spread out the tokens within that cluster over the buckets. The authors motivation for this is:</p>
<blockquote>
<p>very similar tokens need fine distinctions which requires more model capacity (hence assigning to different experts)</p>
</blockquote>
<p>
One final version they try is to hash part of the weight matrix for the feed forward network: \(B(\text{relu}(A(h)))\).
</p>
<div>
$$
v = \text{relu}([A_{k_1}(h), ..., A_{k_N}(h)]), FFN(h) = [B_{k_1}(v), ..., B_{k_n}(v)] 
$$
</div>
<p>
Where, \(k_i\) is determined by the hash: \(k_i = \text{hash}_i(x)\)
</p>
<p class="cssclass1 cssclass2">
  <img src="./images/MoE/Screenshot_2023-09-30_at_2.08.53_PM.png" alt="Logo" title="Logo title">
  <p style="color:grey;font-size: 1vw;"></em>
</p>
<p class="cssclass1 cssclass2">
  <img src="./images/MoE/Screenshot_2023-09-30_at_2.09.08_PM.png" alt="Logo" title="Logo title">
  <p style="color:grey;font-size: 1vw;"></em>
</p>
<p class="cssclass1 cssclass2">
  <img src="./images/MoE/Screenshot_2023-09-30_at_2.09.53_PM.png" alt="Logo" title="Logo title">
  <p style="color:grey;font-size: 1vw;"></em>
</p>
<h3 id="dselect-k-differentiable-selection-in-the-mixture-of-experts-with-applications-to-multi-task-learninghttpsarxivorgabs210603760"><strong><a href="https://arxiv.org/abs/2106.03760">DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning</a></strong><a hidden class="anchor" aria-hidden="true" href="#dselect-k-differentiable-selection-in-the-mixture-of-experts-with-applications-to-multi-task-learninghttpsarxivorgabs210603760">#</a></h3>
<p>So in most of the other works in this page we often use a top k select over the gating values. The authors of this paper propose that that could lead to instabilities during training, because the loss landscape is no longer smooth. So to reiterate, the prior MoE models often are equivalent to solving:</p>
<div>
$$
\underset{f_1, ..., f_n, w}{\min}\frac{1}{N}\sum_{(x,y)\in D}\ell(y, \sum_{i=1}^{n}f_i(x)w_i)\\
\text{s.t.}\,\,\, ||w||_0\leq k \\
\sum_{i=1}^n w_i = 1, w \geq 0
$$
</div>
<p>
So here the \(L_0\) norm constraint is what makes it difficult for our usual gradient based optimizers. Consequently, the contribution of this work is to convert this into a unconstrained optimization problem. Pretty cool! 
</p>
<div>
$$
r(z)_i = \prod_{j\in B(i-1)}(z_j)\prod_{j\in [m] \backslash B(i-1)}(1-z_j)
$$
</div>
<p>
This formula can map binary numbers to one hot vectors. For example, if \(z = [1,0]\) then \(r(z)_2 = 1\), since I am using 0 indexing. Thus we can use this to obtain a mixture over k experts with a stack of k binary numbers which is \(Z\).  
</p>
<div>
$$
q(\alpha, Z) = \sum_{i=1}^k\sigma(\alpha)_ir(z^{(i)})
$$
</div>
<p>So our new optimization problem becomes:</p>
<div>
$$
\underset{f_1, ..., f_n, \alpha, Z}{min} \frac{1}{N}\sum_{(x,y)\in D} \ell(y, \sum_{i]1}^nf_i(x)q(\alpha, Z)_i)\\
z^{(i)}\in \{0,1\}^m, i\in[k]
$$
</div>
<p>
But this is still not that useful because \(z^{(i)}\)  is still a binary vector which becomes a combinatorial optimization problem which is not what we want. So instead lets relax \(z^{(i)}\) to be continuous and we can do that with the following. 
\(S(t)\) smooth function that can exactly equal 0, 1. 
</p>
<div>
$$
\tilde{q}(\alpha, Z) \coloneqq q(\alpha, S(Z)) = \sum_{i=1}^k\sigma(\alpha)_ir(S(z^{(i)}))
$$
</div>
<div>
$$
\underset{f_1, ..., f_n, \alpha, Z}{min} \frac{1}{N}\sum_{(x,y)\in D} \ell(y, \sum_{i=1}^nf_i(x)\tilde{q}(\alpha, Z)_i)  + \lambda \Omega(Z)\\
$$
</div>
<p>
The entropy isn’t directly calculated on \(Z\) but \(\Omega(Z)\coloneqq \sum_{i=1}^kh(r(S(z^{(i)})))\), where $h$ is an entropy function. The authors state that the entropy regularization isn’t needed because empirically the \(z^{(i)}\) will become a binary vector, but for faster convergence the entropy term helps. So \(\alpha, Z\) does not depend on \(x\) , but you can easily do that as well via a linear transformation. 
</p>
<h3 id="base-layers-simplifying-training-of-large-sparse-modelshttpsproceedingsmlrpressv139lewis21ahtml"><strong><a href="https://proceedings.mlr.press/v139/lewis21a.html">BASE Layers: Simplifying Training of Large, Sparse Models</a></strong><a hidden class="anchor" aria-hidden="true" href="#base-layers-simplifying-training-of-large-sparse-modelshttpsproceedingsmlrpressv139lewis21ahtml">#</a></h3>
<p class="cssclass1 cssclass2">
  <img src="./images/MoE/Screenshot_2023-10-02_at_9.17.37_AM.png" alt="Logo" title="Logo title">
  <p style="color:grey;font-size: 1vw;"></em>
</p>
<p>
This paper has a similar setup to the previous paper with some key differences. So to go over their notation, they have \(E\) experts, and each one is denoted by \(f_e\) and its learnable representation \(w_e\in \mathbb{R}^D\) to allow us to to routing. \(h_t\)  is the token embedding and \(a_t \in \{0,..., E\}\) is the assignment of the token to expert. So the overall model takes the following form:
</p>
</div>
$$
\sigma(h_t\cdot w_{a_t})f_{a_t}(h_t) +h_t
$$
</div>
<p>The assignment during training and testing is different: the authors write that</p>
<blockquote>
<p>During training, we maximize model throughput by assigning an equal number of tokens to each expert. At test time, we simply assign each token to its highest scoring expert.</p>
</blockquote>
<p>So during training they solve the well studied assignment problem</p>
<div>
$$
\text{maximize}\sum_th_t\cdot w_{a_t}\\
s.t. \forall e \sum_{t=1}^T \mathbb{1}_{a_t=e} = \frac{T}{E}
$$
</div>
<p>
Here \(T\) is the number of tokens. One potential algorithm to solve this is the famous Hungarian matching one, but that is \(O(n^3)\) and not parallelizable. Instead the authors use a different Auction Algorithm (Bertsekas et al. 1922) which is “which is more easily parallelizable on GPUs than the Hungarian Algorithm”. 
</p>
<p>One important point is that since the partition of the dataset over workers would not be IID, they randomly shuffle the tokens across workers before calculating the assignments.</p>
<p class="cssclass1 cssclass2">
  <img src="./images/MoE/Screenshot_2023-10-02_at_9.19.00_AM.png" alt="Logo" title="Logo title">
  <p style="color:grey;font-size: 1vw;"></em>
</p>
<p class="cssclass1 cssclass2">
  <img src="./images/MoE/Screenshot_2023-10-02_at_9.19.11_AM.png" alt="Logo" title="Logo title">
  <p style="color:grey;font-size: 1vw;"></em>
</p>
<h3 id="mixture-of-experts-with-expert-choice-routinghttpsarxivorgpdf220209368pdf"><a href="https://arxiv.org/pdf/2202.09368.pdf">Mixture-of-Experts with Expert Choice Routing</a><a hidden class="anchor" aria-hidden="true" href="#mixture-of-experts-with-expert-choice-routinghttpsarxivorgpdf220209368pdf">#</a></h3>
<p class="cssclass1 cssclass2">
  <img src="./images/MoE/Screenshot_2023-10-01_at_2.06.57_AM.png" alt="Logo" title="Logo title">
  <p style="color:grey;font-size: 1vw;"></em>
</p>
<p>The authors start off by highlighting some of the previous problems with MoE models where the routing function decides which for each token, which expert to route it to. These problems revolve around load imbalance, and as we have seen so far, there are multiple different heuristics / regularizations to encourage more uniform load over the experts. So to be concise, here is how they implement their method:</p>
<div>
$$
S = \text{softmax}(XW_g), S\in\mathbb{R}^{n\times e} \\
G,I = \text{TopK}(S^T, k), P = \text{Onehot(I)}
$$
</div>
<p>
\(X \in \mathbb{R}^{n \times d}\), and \(I\) is an index matrix, and G is the weights of the selection. So then a permutation matrix \(P \in \mathbb{R}^{e \times k \times n}\) is calculated based on \(I\) to reshuffle the indicies such that \(X_{in} \in \mathbb{R}^{e \times k \times d}= PX\)  allows you to index the tokens per expert. Then the FFN and reverse shuffle/weighting is defined as: 
</p>
<div>
$$
\forall i, X_e[i] = \text{GeLU}(X_{in}[i]W_1[i])W_2[i]^T\\
X_{out}[l,d] = \sum_{i,j}P[i,j,l]G[i,j]X_e[i,j,d]
$$
</div>
Notice here that there is nothing to prevent experts taking in the same tokens. While it seems that a more even spread of tokens better utilizes the model’s capacity, they introduce a entropy regularization in the form of: 
<div>
$$
\max_{A}{S^T \cdot A + \lambda H(A)}\\ 
\text{s.t}\,\,\, \forall i: \sum_{j'}A[i,j']=k; \,\,\, \forall j: \sum_{i'}A[i',j]\leq b; \forall i,j: \,\, 0\leq A[i,j]\leq 1
$$
</div>
<p>
Where \(H(A)\) calculates the entropy. So this is a entropy regularized linear program, and after obtaining \(A\)
, instead of \(S^T\) the top k is calculated using \(A\). So what this allows is, if \(S^T’\)s top k are rather skewed towards a small set of tokens, depending on how much larger the top k values are compared to the rest, and \(lambda\), A can reselect a more uniform set of tokens. 
</p>
<p class="cssclass1 cssclass2">
  <img src="./images/MoE/Screenshot_2023-10-01_at_10.08.56_AM.png" alt="Logo" title="Logo title">
  <p style="color:grey;font-size: 1vw;"></em>
</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://minyoungg.github.io/mlscale/tags/first.html">first</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://minyoungg.github.io/mlscale/posts/critical-batch-size.html">
    <span class="title">« Prev Page</span>
    <br>
    <span>Critical batch-size in deep learning</span>
  </a>
  <a class="next" href="https://minyoungg.github.io/mlscale/posts/speculative-decoding.html">
    <span class="title">Next Page »</span>
    <br>
    <span>Speculative decoding</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Mixture of Experts (MoE) on twitter"
        href="https://twitter.com/intent/tweet/?text=Mixture%20of%20Experts%20%28MoE%29&amp;url=https%3a%2f%2fminyoungg.github.io%2fmlscale%2fposts%2fmoe.html&amp;hashtags=first">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Mixture of Experts (MoE) on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fminyoungg.github.io%2fmlscale%2fposts%2fmoe.html&amp;title=Mixture%20of%20Experts%20%28MoE%29&amp;summary=Mixture%20of%20Experts%20%28MoE%29&amp;source=https%3a%2f%2fminyoungg.github.io%2fmlscale%2fposts%2fmoe.html">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Mixture of Experts (MoE) on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fminyoungg.github.io%2fmlscale%2fposts%2fmoe.html&title=Mixture%20of%20Experts%20%28MoE%29">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Mixture of Experts (MoE) on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fminyoungg.github.io%2fmlscale%2fposts%2fmoe.html">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Mixture of Experts (MoE) on whatsapp"
        href="https://api.whatsapp.com/send?text=Mixture%20of%20Experts%20%28MoE%29%20-%20https%3a%2f%2fminyoungg.github.io%2fmlscale%2fposts%2fmoe.html">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Mixture of Experts (MoE) on telegram"
        href="https://telegram.me/share/url?text=Mixture%20of%20Experts%20%28MoE%29&amp;url=https%3a%2f%2fminyoungg.github.io%2fmlscale%2fposts%2fmoe.html">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://minyoungg.github.io/mlscale/"></a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
