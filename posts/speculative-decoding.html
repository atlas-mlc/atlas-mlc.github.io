<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Speculative decoding | scale &#43; algorithms</title>
<meta name="keywords" content="first" />
<meta name="description" content="add description for the article">
<meta name="author" content="
Author: &nbspAni Nrusimha
&nbsp | &nbsp Editor: &nbspMinyoung Huh">
<link rel="canonical" href="https://canonical.url/to/page" />
<link crossorigin="anonymous" href="https://minyoungg.github.io/mlscale/assets/css/stylesheet.min.1c5241cc5c31e8a1af5a56caa332bfa60cee35277d8fb31c7063ca9ed7258093.css" integrity="" rel="preload stylesheet" as="style">
<link rel="icon" href="https://minyoungg.github.io/mlscale/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://minyoungg.github.io/mlscale/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://minyoungg.github.io/mlscale/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://minyoungg.github.io/mlscale/apple-touch-icon.png">
<link rel="mask-icon" href="https://minyoungg.github.io/mlscale/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.92.2" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Speculative decoding" />
<meta property="og:description" content="add description for the article" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://minyoungg.github.io/mlscale/posts/speculative-decoding.html" /><meta property="og:image" content="https://minyoungg.github.io/mlscale/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-30T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2023-09-30T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://minyoungg.github.io/mlscale/papermod-cover.png"/>

<meta name="twitter:title" content="Speculative decoding"/>
<meta name="twitter:description" content="add description for the article"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Speculative decoding",
      "item": "https://minyoungg.github.io/mlscale/posts/speculative-decoding.html"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Speculative decoding",
  "name": "Speculative decoding",
  "description": "add description for the article",
  "keywords": [
    "first"
  ],
  "articleBody": "   captions are tangents unimportant to the rest of the article.\nSo you want to use a large language model on device Only problem. Despite only running a sequence at a time, your model takes forever to run. What gives? You check nvidia-smi, and you are at 100% utilization. Perhaps you go even further. You calculate the number of floating point operations that your model performs, and divide by the number of operations that your GPU can do per second. You find an absurd number: off by several orders of magnitude. Is nvidia-smi lying to you? How do you estimate how fast your model will run? How do you make your model faster without changing your outputs at all? The answers to these questions are:\n technically no, but basically yes you find the bottleneck, which for batch size one1 inference is memory movement for weight data. you get creative, and use something like speculative decoding. ​  The problem Machine learning, from a systems perspective, is matrix multiplication with some additional spice. GPUs are like a big company, and split up the matrix computation into many small parts, and assign teams of workers to compute on each part. To vastly oversimplify a complex structure1, each team has a little bit of shared memory and a bunch of workers which have an absolutely tiny amount of registers in addition to the shared memory 2. The GPU itself has a huge buffer of memory that is globally accessible but far away from the individual SMs. ​ While the size of the global buffer can be as large as 80 GB on a A100 or H100, the size of the shared memory is on the order of hundreds of kilobytes per “team”3. Each SM can only store the weights (model parameters) and activations (model intermediate computations) needed for its immediate subtask, with the size of the subtasks3 carefully selected to fit into this small amount of memory. ​ This means that generating one token requires loading the entire model from main memory to shared memory at least once. The cost of this memory movements dwarfs the cost of computation. ​\nSide note: why does nvidia-smi lie? Nvidia-smi’s utilization takes into account how much of the GPU is being used, and has essentially on information on how effectively the GPU is being used. If you don’t have 99-100% utilization in nvidia-smi, parts of the GPU have nothing to do. ​ As any grad student knows, working a lot does not mean getting a lot of work done. nvidia-smi is measuring the former. The workers are spending the vast majority of their time waiting for stuff, but they have a task to do. As a result, nvidia-smi reports utilization at 100% ​\nLets get creative ​ This reality leads to an interesting implication. We have “free compute” - we could multiply the amount of computation we do without meaningfully increasing the amount of time it takes. ​ To make a long story short, this is the first core insight that leads to speculative decoding. For LLMs specifically, this insight explains why generating the logits of a short sentence will take around the same amount of time as generating a single word. Both involving multiplying all the weight matrices by some activations, and both the cost of memory movement for those activations and the cost of the multiplication pale in comparison to the cost of moving the weights. ​ This means that if you are able to reliably and quickly generate good candidate words for a sentence, you can verify the quality of the sentence much faster than generating it would have taken. The second key insight is fairly obvious: we already have small models that do a decent job of approximating larger ones! Use your 1B parameter model to accelerate your 7B, or 7B to accelerate 70B. ​ Once you realize these two key ideas, the rest is math to make sure that the results from your scheme exactly match the outputs from the original model. ​\nThe math ​ See the original paper for a detailed explanation. A short version is below. ​\n Let \\(p(x)\\) be the large model probability distribution, and \\(q(x)\\) be the smaller one. We are going to do a variant of something called rejection sampling.\nSample \\(n\\) tokens from \\(q(x)\\), called \\(x_1 ... x_n\\).  Generate the logits of the \\(n\\) tokens from \\(p(x)\\) For each token \\(x_i\\), decide whether to accept it based on the following criteria: If \\(q(x_i) Otherwise, sample a random number \\(r\\) from \\([0,1]\\) and compare it to some value \\(v\\). I will explain how to calculate \\(v\\) later. If it is less than \\(v\\), accept anyways If it was greater than \\(v\\), reject, break the loop, and sample the current token from the distribution \\(p(x)-q(x)\\) [4]      That's it! \\(v\\) isn't even that hard to calculate: just take \\(\\frac{p(x_i)}{q(x_i)}\\). [5]  (Comment references 4 5)\n​\nPractical implementation In practice, we have a tradeoff between the speed of our approximation model and the fidelity of its outputs. Small models This paper has some fun ideas on how you might implement the algorithm in a real world setting: have a bunch of potential completions, and have stages of speculative decoding, where the simplest models capture any incredibly obvious patterns and the medium size models capture more nuanced combinations. ​\nPotential research directions ​ Send me a link if you find a paper on this or an email if you want to chat about it! Unfortunately booked at the moment but I think these are all good ideas. ​\n Contrastive staged speculative decoding. You can approximate a big model with a medium size model. Try approximating the difference between a big model and a medium model with the difference between the outputs of a medium model and a small model! Leverage the benefits of contrastive decoding while still being speculative! Early stopping / AlBERT style speculative decoding. What if the outputs of your q(x) would save you time for computing p(x)? This is achievable as long as your small model and large model share parameters. Two easy ways to make this work are to use something like early stopping or AlBERT. Distillation to improve the fidelity of small models to large models! Actually, if you find good papers for LLM (1B model student and teacher) distillation, send them my way. Integrate PagedAttention into a branched / staged system. I think its actually extremely helpful in this use case to handle the shared prefill.    the cuda documentation is actually very good. I know this sounds like I am recommending a torture device, but the first few sections provide a great overview of the architecture and abstractions necessary if you want to go deeper in understanding the topics I touch on today ↩︎\n because hell is empty and all the devils are here this team of workers is called a “streaming multiprocessor”, or SM by nvidia. This is an issue because the memory that the members of the team is called “shared memory”, abbreviated “SM” by everyone but nvidia. ​ ↩︎\n in reality there is a complex corporate structure in the GPU, i.e. there are multiple layers of hierarchy on both the hardware and software side. One worker is a thread. A GPU is divided into GPU Processing Cluster, each of which can contain a thread block cluster. The thread block cluster is subdivided into thread blocks, which each run on a streaming multiprocessor. The individual thread blocks are subdivided into warps, which consist of a set of threads which all run at the same time. ​ ↩︎\n Yes this isn’t a probability distribution, yes you need to normalize it. The normalization makes the algorithm look more complicated than it is, and while this is wrong it is easier to understand. I wrote the entire algorithm without one division sign! ↩︎\n Proving this is actually equivalent is a good way to really understand the algorithm. Ideally you can do so with less hand waving than the paper itself. ↩︎\n   ",
  "wordCount" : "1330",
  "inLanguage": "en",
  "datePublished": "2023-09-30T00:00:00Z",
  "dateModified": "2023-09-30T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Ani Nrusimha"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://minyoungg.github.io/mlscale/posts/speculative-decoding.html"
  },
  "publisher": {
    "@type": "Organization",
    "name": "scale + algorithms",
    "logo": {
      "@type": "ImageObject",
      "url": "https://minyoungg.github.io/mlscale/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://minyoungg.github.io/mlscale/" accesskey="h" title="scale &#43; algorithms (Alt + H)">scale &#43; algorithms</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://minyoungg.github.io/mlscale/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://minyoungg.github.io/mlscale/">Home</a></div>
    <h1 class="post-title">
      Speculative decoding
    </h1>
    <div class="post-description">
      add description for the article
    </div>
    <div class="post-meta"><span title='2023-09-30 00:00:00 +0000 UTC'>September 30, 2023</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;
Author: &nbspAni Nrusimha
&nbsp | &nbsp Editor: &nbspMinyoung Huh

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#so-you-want-to-use-a-large-language-model-on-device" aria-label="So you want to use a large language model on device">So you want to use a large language model on device</a></li>
                <li>
                    <a href="#the-problem" aria-label="The problem">The problem</a><ul>
                        
                <li>
                    <a href="#side-note-why-does-nvidia-smi-lie" aria-label="Side note: why does nvidia-smi lie?">Side note: why does nvidia-smi lie?</a></li></ul>
                </li>
                <li>
                    <a href="#lets-get-creative" aria-label="Lets get creative">Lets get creative</a></li>
                <li>
                    <a href="#the-math" aria-label="The math">The math</a></li>
                <li>
                    <a href="#practical-implementation" aria-label="Practical implementation">Practical implementation</a></li>
                <li>
                    <a href="#potential-research-directions" aria-label="Potential research directions">Potential research directions</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content">

<!-- KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>


<p><em>captions are tangents unimportant to the rest of the article.</em></p>
<h3 id="so-you-want-to-use-a-large-language-model-on-device">So you want to use a large language model on device<a hidden class="anchor" aria-hidden="true" href="#so-you-want-to-use-a-large-language-model-on-device">#</a></h3>
<p>Only problem. Despite only running a sequence at a time, your model takes <em>forever</em> to run.
What gives? You check nvidia-smi, and you are at 100% utilization.
Perhaps you go even further. You calculate the number of floating point operations that your model performs, and divide by the number of operations that your GPU can do per second. You find an absurd number: off by several orders of magnitude.
Is nvidia-smi lying to you? How do you estimate how fast your model will run? How do you make your model faster without changing your outputs <em>at all</em>?
The answers to these questions are:</p>
<ol>
<li>technically no, but basically yes</li>
<li>you find the bottleneck, which for batch size <cite>one<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></cite> inference is memory movement for weight data.</li>
<li>you get creative, and use something like speculative decoding.
​</li>
</ol>
<h3 id="the-problem">The problem<a hidden class="anchor" aria-hidden="true" href="#the-problem">#</a></h3>
<p>Machine learning, from a systems perspective, is matrix multiplication with some additional spice. GPUs are like a big company, and split up the matrix computation into many small parts, and assign teams of workers to compute on each part. To vastly oversimplify a complex structure<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, each team has a little bit of shared memory and a bunch of workers which have an absolutely tiny amount of registers in addition to the shared memory <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. The GPU itself has a huge buffer of memory that is globally accessible but far away from the individual SMs.
​
While the size of the global buffer can be as large as 80 GB on a A100 or H100, the size of the shared memory is on the order of hundreds of kilobytes per &ldquo;team&rdquo;<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. Each SM can only store the weights (model parameters) and activations (model intermediate computations) needed for its immediate subtask, with the size of the subtasks<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> carefully selected to fit into this small amount of memory.
​
This means that generating one token requires loading the entire model from main memory to shared memory at least once. The cost of this memory movements dwarfs the cost of computation.
​</p>
<h4 id="side-note-why-does-nvidia-smi-lie">Side note: why does nvidia-smi lie?<a hidden class="anchor" aria-hidden="true" href="#side-note-why-does-nvidia-smi-lie">#</a></h4>
<p>Nvidia-smi&rsquo;s utilization takes into account how much of the GPU is <em>being used</em>, and has essentially on information on how effectively the GPU is being used. If you don&rsquo;t have 99-100% utilization in nvidia-smi, parts of the GPU have nothing to do.
​
As any grad student knows, working a lot does not mean getting a lot of work done. nvidia-smi is measuring the former. The workers are spending the vast majority of their time waiting for stuff, but they have a task to do. As a result, nvidia-smi reports utilization at 100%
​</p>
<h3 id="lets-get-creative">Lets get creative<a hidden class="anchor" aria-hidden="true" href="#lets-get-creative">#</a></h3>
<p>​
This reality leads to an interesting implication. We have &ldquo;free compute&rdquo; - we could multiply the amount of computation we do without meaningfully increasing the amount of time it takes.
​
To make a long story short, this is the first core insight that leads to speculative decoding. For LLMs specifically, this insight explains why generating the logits of a short sentence will take around the same amount of time as generating a single word. Both involving multiplying all the weight matrices by some activations, and both the cost of memory movement for those activations and the cost of the multiplication pale in comparison to the cost of moving the weights.
​
This means that if you are able to reliably and quickly generate good candidate words for a sentence, you can verify the quality of the sentence much faster than generating it would have taken. The second key insight is fairly obvious: we already have small models that do a decent job of approximating larger ones! Use your 1B parameter model to accelerate your 7B, or 7B to accelerate 70B.
​
Once you realize these two key ideas, the rest is math to make sure that the results from your scheme <em>exactly</em> match the outputs from the original model.
​</p>
<h3 id="the-math">The math<a hidden class="anchor" aria-hidden="true" href="#the-math">#</a></h3>
<p>​
See <a href="https://arxiv.org/abs/2211.17192">the original paper</a> for a detailed explanation. A short version is below.
​</p>
<div style="background-color: #6c757d; padding: 15px 15px 15px 25px; border: 1px solid #e0e0e0; border-radius: 5px; font-family: 'Consolas', monospace; font-size: 0.9em">
<p> Let \(p(x)\) be the large model probability distribution, and \(q(x)\) be the smaller one. We are going to do a variant of something called <a href="https://en.wikipedia.org/wiki/Rejection_sampling">rejection sampling</a>.</p>
<ol style="padding-left: 25px;">
    <li>Sample \(n\) tokens from \(q(x)\), called \(x_1 ... x_n\). </li>
    <li>Generate the logits of the \(n\) tokens from \(p(x)\)</li>
    <li>For each token \(x_i\), decide whether to accept it based on the following criteria:
        <ol style="padding-left: 25px;">
            <li>If \(q(x_i) < p(x_i)\), accept and move to the next token</li>
            <li>Otherwise, sample a random number \(r\) from \([0,1]\) and compare it to some value \(v\). I will explain how to calculate \(v\) later.
                <ol style="padding-left: 25px;">
                    <li>If it is less than \(v\), accept anyways</li>
                    <li>If it was greater than \(v\), reject, break the loop, and sample the current token from the distribution \(p(x)-q(x)\) <a href="#note4" id="ref4">[4]</a></li>
                </ol>
            </li>
        </ol>
    </li>
</ol>
<p>That's it! \(v\) isn't even that hard to calculate: just take \(\frac{p(x_i)}{q(x_i)}\). <a href="#note5" id="ref5">[5]</a></li> </p>
</div>
<p>(Comment references <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>)</p>
<p>​</p>
<h3 id="practical-implementation">Practical implementation<a hidden class="anchor" aria-hidden="true" href="#practical-implementation">#</a></h3>
<p>In practice, we have a tradeoff between the speed of our approximation model and the fidelity of its outputs. Small models
<a href="https://arxiv.org/abs/2308.04623">This paper</a> has some fun ideas on how you might implement the algorithm in a real world setting: have a bunch of potential completions, and have stages of speculative decoding, where the simplest models capture any incredibly obvious patterns and the medium size models capture more nuanced combinations.
​</p>
<h3 id="potential-research-directions">Potential research directions<a hidden class="anchor" aria-hidden="true" href="#potential-research-directions">#</a></h3>
<p>​
Send me a link if you find a paper on this or an email if you want to chat about it! Unfortunately booked at the moment but I think these are all good ideas.
​</p>
<ol>
<li>Contrastive staged speculative decoding. You can approximate a big model with a medium size model. Try approximating the difference between a big model and a medium model with the difference between the outputs of a medium model and a small model! Leverage the benefits of contrastive decoding while still being speculative!</li>
<li>Early stopping / AlBERT style speculative decoding. What if the outputs of your q(x) would save you time for computing p(x)? This is achievable as long as your small model and large model share parameters. Two easy ways to make this work are to use something like early stopping or AlBERT.</li>
<li>Distillation to improve the fidelity of small models to large models! Actually, if you find good papers for LLM (&gt;1B model student and teacher) distillation, send them my way.</li>
<li>Integrate PagedAttention into a branched / staged system. I think its actually extremely helpful in this use case to handle the shared prefill.</li>
</ol>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>the cuda documentation is actually very good. I know this sounds like I am recommending a torture device, but the first few sections provide a great overview of the architecture and abstractions necessary if you want to go deeper in understanding the topics I touch on today&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>because hell is empty and all the devils are here this team of workers is called a &ldquo;streaming multiprocessor&rdquo;, or SM by nvidia. This is an issue because the memory that the members of the team is called &ldquo;shared memory&rdquo;, abbreviated &ldquo;SM&rdquo; by everyone but nvidia.
​&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>in reality there is a complex corporate structure in the GPU, i.e. there are multiple layers of hierarchy on both the hardware and software side.  One worker is a thread. A GPU is divided into GPU Processing Cluster, each of which can contain a thread block cluster. The thread block cluster is subdivided into thread blocks, which each run on a streaming multiprocessor. The individual thread blocks are subdivided into warps, which consist of a set of threads which all run at the same time.
​&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>Yes this isn&rsquo;t a probability distribution, yes you need to normalize it. The normalization makes the algorithm look more complicated than it is, and while this is wrong it is easier to understand. I wrote the entire algorithm without one division sign!&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>Proving this is actually equivalent is a good way to really understand the algorithm. Ideally you can do so with less hand waving than the paper itself.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://minyoungg.github.io/mlscale/tags/first.html">first</a></li>
    </ul>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Speculative decoding on twitter"
        href="https://twitter.com/intent/tweet/?text=Speculative%20decoding&amp;url=https%3a%2f%2fminyoungg.github.io%2fmlscale%2fposts%2fspeculative-decoding.html&amp;hashtags=first">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Speculative decoding on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fminyoungg.github.io%2fmlscale%2fposts%2fspeculative-decoding.html&amp;title=Speculative%20decoding&amp;summary=Speculative%20decoding&amp;source=https%3a%2f%2fminyoungg.github.io%2fmlscale%2fposts%2fspeculative-decoding.html">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Speculative decoding on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fminyoungg.github.io%2fmlscale%2fposts%2fspeculative-decoding.html&title=Speculative%20decoding">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Speculative decoding on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fminyoungg.github.io%2fmlscale%2fposts%2fspeculative-decoding.html">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Speculative decoding on whatsapp"
        href="https://api.whatsapp.com/send?text=Speculative%20decoding%20-%20https%3a%2f%2fminyoungg.github.io%2fmlscale%2fposts%2fspeculative-decoding.html">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Speculative decoding on telegram"
        href="https://telegram.me/share/url?text=Speculative%20decoding&amp;url=https%3a%2f%2fminyoungg.github.io%2fmlscale%2fposts%2fspeculative-decoding.html">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://minyoungg.github.io/mlscale/">scale &#43; algorithms</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
